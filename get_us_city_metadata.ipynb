{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapy of US City Metadata\n",
    "This project is aiming in web scrapy by querying approximately **27540** city metadata from [uszip.com](https://www.uszip.com/). For each city, we want know their basci information, households, industry, occupation, etc., and output them to a csv file. \n",
    "\n",
    "There are two phases in this projects. The first one is use all state code (eg. CA) to get all zipcodes, and save this zip code file to a csv for further use. \n",
    "\n",
    "In the second phase, we will do a data processing. Since a zip code could be located to several cities, it's important to know which city is we are targeting given the zip code. Therefore, we use the city names from **zip-code-city-state-county_table.csv** and retrieve the default city name from it. In the end, we designed a spider to query all metadata of all cities and output the result with headers and each city as a row to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch All ZIP Codes Under a State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_codes = [\n",
    "    'AL', \n",
    "    'AK', \n",
    "    'AZ', \n",
    "    'AR', \n",
    "    'CA', \n",
    "    'CO', \n",
    "    'CT', \n",
    "    'DE', \n",
    "    'DC',\n",
    "    'FL', \n",
    "    'GA', \n",
    "    'HI', \n",
    "    'ID', \n",
    "    'IL', \n",
    "    'IN', \n",
    "    'IA', \n",
    "    'KS', \n",
    "    'KY', \n",
    "    'LA', \n",
    "    'ME', \n",
    "    'MD', \n",
    "    'MA', \n",
    "    'MI', \n",
    "    'MN', \n",
    "    'MS', \n",
    "    'MO', \n",
    "    'MT', \n",
    "    'NE', \n",
    "    'NV', \n",
    "    'NH', \n",
    "    'NJ', \n",
    "    'NM', \n",
    "    'NY', \n",
    "    'NC', \n",
    "    'ND', \n",
    "    'OH', \n",
    "    'OK', \n",
    "    'OR', \n",
    "    'PA', \n",
    "    'PR', \n",
    "    'RI', \n",
    "    'SC', \n",
    "    'SD', \n",
    "    'TN', \n",
    "    'TX', \n",
    "    'UT', \n",
    "    'VT', \n",
    "    'VA', \n",
    "    'VI', \n",
    "    'WA', \n",
    "    'WV',\n",
    "    'WI',\n",
    "    'WY'\n",
    "]\n",
    "us_zip_base_url = 'https://www.uszip.com'\n",
    "us_zip_start_urls = [f'{us_zip_base_url}/state/{state_code}' for state_code in state_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{\n",
    "    \"<state_code>\": {\n",
    "        \"<county_name>\": [<zip_code>]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "state_to_zips_dict = defaultdict(lambda: defaultdict(lambda: []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "zip_code_format = r'^\\d{5}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve zip codes under a state and store them to state_to_zips_dict\n",
    "\"\"\"\n",
    "class USZipCodeSpider(scrapy.Spider):\n",
    "    name = 'us_city_zipcode'\n",
    "    start_urls = us_zip_start_urls\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for state in response.xpath('//*[@class=\"table tdata\"]'):\n",
    "            o.append(state)\n",
    "            county_list = state.xpath('//tbody//tr')\n",
    "            for county in county_list:\n",
    "                county_name = county.xpath('td[2]//text()')[0].get().strip(',')\n",
    "                state_code = county.xpath('td[2]//text()')[1].get().strip()\n",
    "                \n",
    "                zip_codes = [code.get() for code in county.xpath('td[3]//text()')]\n",
    "                valid_zip_codes = []\n",
    "                for zip_code in zip_codes:\n",
    "                    if re.search(zip_code_format, zip_code):\n",
    "                        valid_zip_codes.append(zip_code)\n",
    "\n",
    "                state_to_zips_dict[state_code][county_name].extend(valid_zip_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File state_to_zip.csv already exist\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "\n",
    "if not path.exists('state_to_zip.csv'):\n",
    "    print('Start to parse zipcodes')\n",
    "    fetch_zip_code_process = CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})\n",
    "    fetch_zip_code_process.crawl(USZipCodeSpider)\n",
    "    fetch_zip_code_process.start()\n",
    "    \n",
    "    # write results to csv file\n",
    "    # format: <state>, <city>, <zip_code_list>\n",
    "    # example: DE, Camden, 19901-19904-19934 \n",
    "\n",
    "    import csv\n",
    "    with open('state_to_zip.csv', 'w') as f:\n",
    "        f.write(\"State,City,Zip_Code_List\\n\")\n",
    "        for state_code, state_metadata in state_to_zips_dict.items():\n",
    "            print(f'Writing info for state {state_code}')\n",
    "            for city, zip_codes in state_metadata.items():\n",
    "                zip_code_list = '-'.join(zip_codes)\n",
    "                f.write(\"%s,%s,%s\\n\" % (state_code, city, zip_code_list))\n",
    "else:\n",
    "    print('File state_to_zip.csv already exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch County Metadata By Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all available zipcodes from uszip.com\n",
    "import csv\n",
    "zip_codes = set()\n",
    "with open('state_to_zip.csv', 'r') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    reader = next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        # split the zip code and remove empty zipcode before joining them to the list\n",
    "        codes = list(filter(None, row[2].split('-')))\n",
    "        zip_codes.update(codes)\n",
    "zip_codes = list(zip_codes)\n",
    "\n",
    "city_metadata_urls = [f'{us_zip_base_url}/zip/{zip_code}' for zip_code in zip_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33097"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read desired zipcodes, city name and state information\n",
    "zip_code_to_city_state_county = {}\n",
    "with open('zip-code-city-state-county_table.csv', 'r') as f:\n",
    "    for line in csv.DictReader(f): \n",
    "        zip_code = line['zip']\n",
    "        zip_code_to_city_state_county[zip_code] = dict(line)\n",
    "len(zip_code_to_city_state_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" format: \n",
    "{\n",
    "    zipcode_00115: {\n",
    "        \"city_metadata\": {                        # this part is made by us\n",
    "            \"state_id\": 'CA', city: 'Sunnyvale', 'county_name': 'Santa Clara'\n",
    "        }, \n",
    "        basic info\": {}, \"Households\": {}....  # these parts are from uszip.com\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since scrapy cannot do xpath query to a portion of html\n",
    "# we need to use other package for local search\n",
    "from lxml import html\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CityMetadataSpider(scrapy.Spider):\n",
    "    name = 'us_city_metadata'\n",
    "    start_urls = city_metadata_urls\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available_section_titles = ['Basic info', 'Households',\n",
    "                                'Housing units', 'Age', 'Employment status', \n",
    "                               'Occupation', 'Industry',\n",
    "                               'Class of worker', 'Income']\n",
    "    \n",
    "    # parsing from the back to avoid any missing fields\n",
    "    def parse_section(tree):\n",
    "        res = []\n",
    "        title_idx = len(tree.xpath('//dt')) - 1\n",
    "        value_idx = len(tree.xpath('//dd')) - 1\n",
    "        while value_idx >= 0:\n",
    "            name = tree.xpath('//dt')[title_idx].text\n",
    "            if not name: \n",
    "                name = tree.xpath(f'//dt[{title_idx + 1}]/strong/text()')[0]\n",
    "            value = tree.xpath('//dd')[value_idx].text\n",
    "            title_idx -= 1\n",
    "            value_idx -= 1\n",
    "            res.append((name, value))\n",
    "        res.reverse()\n",
    "        return res\n",
    "\n",
    "    def parse(self, response):\n",
    "        # fetch basic info, gender, households, etc.\n",
    "        zip_code = response.xpath('//*[@id=\"content-body\"]/div/div[2]/hgroup/h3/strong/text()').get()\n",
    "        city = response.xpath('//*[@id=\"content-body\"]/div/div[2]/hgroup/h2/strong/text()').get().strip(', ')\n",
    "        state_id = response.xpath('//*[@id=\"content-body\"]/div/div[2]/hgroup/h2/strong/a/text()').get()\n",
    "        \n",
    "        # use city and state_id from our side\n",
    "        if zip_code in zip_code_to_city_state_county:\n",
    "            city = zip_code_to_city_state_county[zip_code]['city']\n",
    "            state_id = zip_code_to_city_state_county[zip_code]['state_id']\n",
    "        city_metadata = [ (\"zip_code\", zip_code), (\"city\", city), (\"state_id\", state_id) ]\n",
    "        \n",
    "        section_infos = OrderedDict({ \"city_metadata\": city_metadata })\n",
    "        \n",
    "        for i, section in enumerate(response.xpath('//*[@id=\"content-body\"]').xpath('//dl')): \n",
    "            # convert the section content to HTML string\n",
    "            # the html package will parse it, and we can do futher analysis\n",
    "            tree = html.fromstring(section.get())\n",
    "            \n",
    "            if i == 0:\n",
    "                section_name = 'Basic info'\n",
    "            else:\n",
    "                section_name = tree.xpath('//h3/text()')[0].strip(\":\")\n",
    "            if section_name in self.available_section_titles:\n",
    "                section_info = CityMetadataSpider.parse_section(tree)\n",
    "                section_infos[section_name] = section_info\n",
    "        output[zip_code] = section_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fetch_metadata_process = CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})\n",
    "fetch_metadata_process.crawl(CityMetadataSpider)\n",
    "fetch_metadata_process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Result to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zip_code',\n",
       " 'city',\n",
       " 'state_id',\n",
       " 'Total population',\n",
       " 'Housing units',\n",
       " 'Land area',\n",
       " 'Density',\n",
       " 'Water area',\n",
       " 'Total households',\n",
       " 'Family households',\n",
       " 'Nonfamily households',\n",
       " 'Average household size',\n",
       " 'Average family size',\n",
       " 'Total housing units',\n",
       " 'Occupied housing units',\n",
       " 'Owner-occupied',\n",
       " 'Renter-occupied',\n",
       " 'Vacant housing units',\n",
       " 'Under 5 years',\n",
       " '5 to 9 years',\n",
       " '10 to 14 years',\n",
       " '15 to 19 years',\n",
       " '20 to 24 years',\n",
       " '25 to 34 years',\n",
       " '35 to 44 years',\n",
       " '45 to 54 years',\n",
       " '55 to 59 years',\n",
       " '60 to 64 years',\n",
       " '65 to 74 years',\n",
       " '75 to 84 years',\n",
       " '85 years and over',\n",
       " 'Median age',\n",
       " 'Population',\n",
       " 'Civilian labor force',\n",
       " 'Employed',\n",
       " 'Unemployed',\n",
       " 'Armed Forces',\n",
       " 'Not in labor force',\n",
       " 'Percent Unemployed',\n",
       " 'Civilian employed population',\n",
       " 'Management / business / science / arts',\n",
       " 'Service',\n",
       " 'Sales / office',\n",
       " 'Natural resources / construction / maintenance',\n",
       " 'Production / transportation / material moving',\n",
       " 'Civilian employed population',\n",
       " 'Agriculture / forestry / fishing / hunting / mining',\n",
       " 'Construction',\n",
       " 'Manufacturing',\n",
       " 'Wholesale trade',\n",
       " 'Retail trade',\n",
       " 'Transportation / warehousing / utilities',\n",
       " 'Information',\n",
       " 'Finance / insurance / real estate / rental / leasing',\n",
       " 'Professional / scientific / management / administrative / waste management services',\n",
       " 'Educational services / health care / social assistance',\n",
       " 'Arts / entertainment / recreation / accommodation / food services',\n",
       " 'Public administration',\n",
       " 'Other services',\n",
       " 'Civilian employed population',\n",
       " 'Private wage & salary workers',\n",
       " 'Government workers',\n",
       " 'Self-employed',\n",
       " 'Unpaid family workers',\n",
       " 'Total households',\n",
       " 'Less than $10,000',\n",
       " '$10,000 to $14,999',\n",
       " '$15,000 to $24,999',\n",
       " '$25,000 to $34,999',\n",
       " '$35,000 to $49,999',\n",
       " '$50,000 to $74,999',\n",
       " '$75,000 to $99,999',\n",
       " '$100,000 to $149,999',\n",
       " '$150,000 to $199,999',\n",
       " '$200,000 or more',\n",
       " 'Median household income',\n",
       " 'Mean household income',\n",
       " 'Percentage of people whose income in the past 12 months is below the poverty level']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get headers from the first city\n",
    "first_city_zip_code = list(output.keys())[0]\n",
    "\n",
    "headers = []\n",
    "city = output[first_city_zip_code]\n",
    "for section in list(city.values()):\n",
    "    for title, value in section:\n",
    "        headers.append(title)\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('city_metadata.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    for city in output.values():\n",
    "        flatten_values = []\n",
    "        for section in city.values():\n",
    "            for _, value in section:\n",
    "                flatten_values.append(value)\n",
    "        writer.writerow(flatten_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
